{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN-Based Heating and Ventilation System."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First asumption: \n",
    "\n",
    "The server temperature is apporximated since I don't have hestorical data of the server temperature. I apporximate the temperature through the multiple linear regression equation:\n",
    "\n",
    "<em>Server temperature</em> =  <em>$ b_{\\mathrm{0}\\;}$ +  $ b_{\\mathrm{1}\\;}$ x Atmospheric temperature +  $ b_{\\mathrm{2}\\;}$ x number of users +  $ b_{\\mathrm{3}\\;}$ x Rate of data transmission.</em>\n",
    "\n",
    "\n",
    " <em>Where:</em> $ b_{\\mathrm{0}\\;}$ ${\\in}$  ${\\rm I\\!R}$, $ b_{\\mathrm{1}\\;} > 0,\\hspace{.1cm}  b_{\\mathrm{2}\\;} > 0,\\hspace{.1cm}  b_{\\mathrm{3}\\;} > 0$. \n",
    "\n",
    "\n",
    "Assuming we performe the multiple linear regression and I get the values : \n",
    "\n",
    "$$\n",
    "b_{\\mathrm{0}\\;} = 0,\\hspace{.2cm}\n",
    "b_{\\mathrm{1}\\;} = 1, \\hspace{.2cm}\n",
    "b_{\\mathrm{2}\\;} = 1.25\\hspace{.2cm}\n",
    "b_{\\mathrm{3}\\;} = 1.25\n",
    "$$\n",
    "\n",
    "then the equation becomes: \n",
    "\n",
    "<em>Server temperature</em> =  <em> Atmospheric temperature +  $ 1.25$ x number of users +  $ 1.25$ x Rate of data transmission.</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second assumption: \n",
    "\n",
    "The cost of energy needed to bring back the server into the optimal temperature are approximated also with a linear regression. \n",
    "\n",
    "$$\n",
    "e_{\\mathrm{hv}\\;} = \\alpha \\vert T_{\\mathrm{t+1}\\;}-T_{\\mathrm{t}\\;}  \\vert + \\beta\n",
    "$$\n",
    "\n",
    "$\\newline$ \n",
    "\n",
    "<em> where: \n",
    "\n",
    "\n",
    "\n",
    "$e_{\\mathrm{hv}\\;}$ : is the energy spent either for heating or ventilation to bring back the server to optimal range of temperature. \n",
    "\n",
    "$\\vert T_{\\mathrm{t+1}\\;}-T_{\\mathrm{t}\\;}  \\vert$ : is the temperature change in the server between t and t+1.\n",
    "\n",
    "$\\newline$ \n",
    "\n",
    "$\\alpha > 0, \\hspace{.2cm} \\beta \\in {\\rm I\\!R}$\n",
    "\n",
    "and indeed we dont't have reel temperature data so I assume that the value after performing the linear regression :\n",
    "\n",
    "$\\alpha = 1 \\hspace{.2cm}and\\hspace{.2cm}  \\beta = 0$.\n",
    "\n",
    "therefore the assumption becomes : $e_{\\mathrm{hv}\\;} = \\alpha \\vert T_{\\mathrm{t+1}\\;}-T_{\\mathrm{t}\\;}  \\vert$.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING LIBRARIES AND PACKAGES.\n",
    "import os\n",
    "import numpy as np\n",
    "import random as rn\n",
    "from src.models import environment\n",
    "from src. models import nn_dropout\n",
    "from src.models import dqn\n",
    "from keras.models import load_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the environement."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            #THE MONTHLY AVERAGE ATMOSPHERIC TEMPERATURE.\n",
    "            #THE OPTIMAL TEMPERATURE RANGE OF THE SERVER, WHICH WE'LL SET AS.\n",
    "            #THE MINIMUM TEMPERATURE, BELOW WHICH THE SERVER FAILS TO OPERATE.\n",
    "            #THE MAXIMUM TEMPERATURE, ABOVE WHICH THE SERVER FAILS TO OPERATE.\n",
    "            #THE MINIMUM NUMBER OF USERS IN THE SERVER.\n",
    "            #THE MAXIMUM NUMBER OF USERS IN THE SERVER.\n",
    "            #THE MAXIMUM CHANGE OF USERS IN THE SERVER PER MINUTE.\n",
    "            #THE MINIMUM RATE OF DATA TRANSMISSION IN THE SERVER.\n",
    "            #THE MAXIMUM RATE OF DATA TRANSMISSION IN THE SERVER.\n",
    "            #THE MAXIMUM CHANGE OF THE RATE OF DATA TRANSMISSION PER MINUTE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing set of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THE TEMPERATURE OF THE SERVER AT A GIVEN MINUTE.\n",
    "#THE NUMBER OF USERS CONNECTED TO THE SERVER AT A GIVEN MINUTE.\n",
    "#THE RATE OF DATA TRANSMISSION AT A GIVEN MINUTE.\n",
    "#THE ENERGY SPENT BY THE AI ONTO THE SERVER (TO COOL IT DOWN OR HEAT IT UP) AT A GIVING MINUTE.\n",
    "#THE ENERGY THAT WOULD BE SPENT BY THE CLASSIC SERVER'S COOLING SYSTEM."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the dropout Neural Network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Deep Q-Learning with Experience Replay."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the AI with Early Stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING SEEDS FOR REPRODUCIBILITY\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "\n",
    "# SETTING THE PARAMETERS\n",
    "epsilon = .3\n",
    "number_actions = 5\n",
    "direction_boundary = (number_actions - 1) / 2\n",
    "number_epochs = 100\n",
    "max_memory = 3000\n",
    "batch_size = 512\n",
    "temperature_step = 1.5\n",
    "\n",
    "# BUILDING THE ENVIRONMENT BY SIMPLY CREATING AN OBJECT OF THE ENVIRONMENT CLASS\n",
    "env = environment.Environment(optimal_temperature = (18.0, 24.0), initial_month = 0, initial_number_users = 20, initial_rate_data = 30)\n",
    "\n",
    "# BUILDING THE BRAIN BY SIMPLY CREATING AN OBJECT OF THE BRAIN CLASS\n",
    "brain = nn_dropout.Brain(learning_rate = 0.00001, number_actions = number_actions)\n",
    "\n",
    "# BUILDING THE DQN MODEL BY SIMPLY CREATING AN OBJECT OF THE DQN CLASS\n",
    "dqn = dqn.DQN(max_memory = max_memory, discount = 0.9)\n",
    "\n",
    "# CHOOSING THE MODE\n",
    "train = True\n",
    "\n",
    "# TRAINING THE AI\n",
    "env.train = train\n",
    "model = brain.model\n",
    "early_stopping = True\n",
    "patience = 10\n",
    "best_total_reward = -np.inf\n",
    "patience_count = 0\n",
    "if (env.train):\n",
    "    # STARTING THE LOOP OVER ALL THE EPOCHS (1 Epoch = 5 Months)\n",
    "    for epoch in range(1, number_epochs):\n",
    "        # INITIALIAZING ALL THE VARIABLES OF BOTH THE ENVIRONMENT AND THE TRAINING LOOP\n",
    "        total_reward = 0\n",
    "        loss = 0.\n",
    "        new_month = np.random.randint(0, 12)\n",
    "        env.reset(new_month = new_month)\n",
    "        game_over = False\n",
    "        current_state, _, _ = env.observe()\n",
    "        timestep = 0\n",
    "        # STARTING THE LOOP OVER ALL THE TIMESTEPS (1 Timestep = 1 Minute) IN ONE EPOCH\n",
    "        while ((not game_over) and timestep <= 5 * 30 * 24 * 60):\n",
    "            # PLAYING THE NEXT ACTION BY EXPLORATION\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = np.random.randint(0, number_actions)\n",
    "                if (action - direction_boundary < 0):\n",
    "                    direction = -1\n",
    "                else:\n",
    "                    direction = 1\n",
    "                energy_ai = abs(action - direction_boundary) * temperature_step\n",
    "            # PLAYING THE NEXT ACTION BY INFERENCE\n",
    "            else:\n",
    "                q_values = model.predict(current_state)\n",
    "                action = np.argmax(q_values[0])\n",
    "                if (action - direction_boundary < 0):\n",
    "                    direction = -1\n",
    "                else:\n",
    "                    direction = 1\n",
    "                energy_ai = abs(action - direction_boundary) * temperature_step\n",
    "            # UPDATING THE ENVIRONMENT AND REACHING THE NEXT STATE\n",
    "            next_state, reward, game_over = env.update_env(direction, energy_ai, ( new_month + int(timestep/(30*24*60)) ) % 12)\n",
    "            total_reward += reward\n",
    "            # STORING THIS NEW TRANSITION INTO THE MEMORY\n",
    "            dqn.remember([current_state, action, reward, next_state], game_over)\n",
    "            # GATHERING IN TWO SEPARATE BATCHES THE INPUTS AND THE TARGETS\n",
    "            inputs, targets = dqn.get_batch(model, batch_size = batch_size)\n",
    "            # COMPUTING THE LOSS OVER THE TWO WHOLE BATCHES OF INPUTS AND TARGETS\n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "            timestep += 1\n",
    "            current_state = next_state\n",
    "        # PRINTING THE TRAINING RESULTS FOR EACH EPOCH\n",
    "        print(\"\\n\")\n",
    "        print(\"Epoch: {:03d}/{:03d}\".format(epoch, number_epochs))\n",
    "        print(\"Total Energy spent with an AI: {:.0f}\".format(env.total_energy_ai))\n",
    "        print(\"Total Energy spent with no AI: {:.0f}\".format(env.total_energy_noai))\n",
    "        # EARLY STOPPING\n",
    "        if (early_stopping):\n",
    "            if (total_reward <= best_total_reward):\n",
    "                patience_count += 1\n",
    "            elif (total_reward > best_total_reward):\n",
    "                best_total_reward = total_reward\n",
    "                patience_count = 0\n",
    "            if (patience_count >= patience):\n",
    "                print(\"Early Stopping\")\n",
    "                break\n",
    "        # SAVING THE MODEL\n",
    "        model.save(\"model.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING THE PARAMETERS\n",
    "number_actions = 5\n",
    "direction_boundary = (number_actions - 1) / 2\n",
    "temperature_step = 1.5\n",
    "\n",
    "# BUILDING THE ENVIRONMENT BY SIMPLY CREATING AN OBJECT OF THE ENVIRONMENT CLASS\n",
    "env = environment.Environment(optimal_temperature = (18.0, 24.0), initial_month = 0, initial_number_users = 20, initial_rate_data = 30)\n",
    "\n",
    "# LOADING A PRE-TRAINED BRAIN\n",
    "model = load_model(\"model.h5\")\n",
    "\n",
    "# CHOOSING THE MODE\n",
    "train = False\n",
    "\n",
    "# RUNNING A 1 YEAR SIMULATION IN INFERENCE MODE\n",
    "env.train = train\n",
    "current_state, _, _ = env.observe()\n",
    "for timestep in range(0, 12 * 30 * 24 * 60):\n",
    "    q_values = model.predict(current_state)\n",
    "    action = np.argmax(q_values[0])\n",
    "    if (action - direction_boundary < 0):\n",
    "        direction = -1\n",
    "    else:\n",
    "        direction = 1\n",
    "    energy_ai = abs(action - direction_boundary) * temperature_step\n",
    "    next_state, reward, game_over = env.update_env(direction, energy_ai, int(timestep / (30 * 24 * 60)))\n",
    "    current_state = next_state\n",
    "\n",
    "# PRINTING THE TRAINING RESULTS FOR EACH EPOCH\n",
    "print(\"\\n\")\n",
    "print(\"Total Energy spent with an AI: {:.0f}\".format(env.total_energy_ai))\n",
    "print(\"Total Energy spent with no AI: {:.0f}\".format(env.total_energy_noai))\n",
    "print(\"ENERGY SAVED: {:.0f} %\".format((env.total_energy_noai - env.total_energy_ai) / env.total_energy_noai * 100))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
